{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db494969",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (CBOW) Implementation\n",
    "\n",
    "This notebook implements the Continuous Bag of Words (CBOW) model for Word2Vec embeddings.\n",
    "\n",
    "### Overview\n",
    "- **CBOW**: Predicts a target word from surrounding context words\n",
    "- **Dataset**: text8 corpus (cleaned Wikipedia articles)\n",
    "- **Framework**: PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f92bb646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.4.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.10.7)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from gensim) (2.3.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing the necessary libraries\n",
    "# Using python -m pip ensures packages are installed for the current kernel's Python version\n",
    "!pip3 install -U gensim matplotlib scikit-learn torch\n",
    "# Note: zipfile is a built-in Python module, no installation needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0a7d370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text8 corpus already exists.\n"
     ]
    }
   ],
   "source": [
    "# Download the text8 corpus (cleaned Wikipedia articles by Matt Mahoney)\n",
    "# Using curl instead of wget (macOS compatible)\n",
    "import os\n",
    "if not os.path.exists('text8'):\n",
    "    if not os.path.exists('text8.zip'):\n",
    "        print(\"Downloading text8 corpus...\")\n",
    "        !curl -O https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\n",
    "    print(\"Extracting text8 corpus...\")\n",
    "    !unzip -q text8.zip\n",
    "    print(\"Download and extraction complete!\")\n",
    "else:\n",
    "    print(\"text8 corpus already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27f7d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "PyTorch version: 2.9.1\n",
      "NumPy version: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "# Configure matplotlib for inline plotting\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "# Standard library imports\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bcec4",
   "metadata": {},
   "source": [
    "### Load and Preprocess the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e148a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in corpus: 17,005,207\n",
      "First 50 words: anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the\n"
     ]
    }
   ],
   "source": [
    "# Load the text8 corpus\n",
    "with open('text8', 'r') as f:\n",
    "    text = f.read()\n",
    "# Split into words\n",
    "words = text.split()\n",
    "print(f\"Total words in corpus: {len(words):,}\")\n",
    "print(f\"First 50 words: {' '.join(words[:50])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e374d80e",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "902dbecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 71,290\n",
      "Total unique words (before filtering): 253,854\n",
      "Words removed (count < 5): 182,564\n",
      "\n",
      "Sample vocabulary words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'english', 'revolution', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary by counting word frequencies\n",
    "# Remove rare words (appearing less than min_count times)\n",
    "min_count = 5\n",
    "word_counts = Counter(words)\n",
    "vocab = {word: count for word, count in word_counts.items() if count >= min_count}\n",
    "# Create word to index and index to word mappings\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab.keys())}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Total unique words (before filtering): {len(word_counts):,}\")\n",
    "print(f\"Words removed (count < {min_count}): {len(word_counts) - vocab_size:,}\")\n",
    "print(f\"\\nSample vocabulary words: {list(vocab.keys())[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ad6d9",
   "metadata": {},
   "source": [
    "### Prepare Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a903d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training pairs...\n",
      "Total training pairs: 16,718,840\n",
      "Sample pair: context=['anarchism', 'originated', 'a', 'term'], target=as\n"
     ]
    }
   ],
   "source": [
    "# Convert words to indices, filtering out words not in vocabulary\n",
    "words_idx = [word_to_idx[word] for word in words if word in word_to_idx]\n",
    "# Create context-target pairs for CBOW\n",
    "# CBOW: context words -> target word\n",
    "window_size = 2  # 2 words before and 2 words after the target\n",
    "def create_cbow_data(words_idx, window_size):\n",
    "    \"\"\"Create (context, target) pairs for CBOW training.\"\"\"\n",
    "    context_target_pairs = []\n",
    "    \n",
    "    for i in range(window_size, len(words_idx) - window_size):\n",
    "        # Context: words around the target\n",
    "        context = words_idx[i-window_size:i] + words_idx[i+1:i+window_size+1]\n",
    "        # Target: word at position i\n",
    "        target = words_idx[i]\n",
    "        context_target_pairs.append((context, target))\n",
    "    \n",
    "    return context_target_pairs\n",
    "\n",
    "# Create training pairs\n",
    "print(\"Creating training pairs...\")\n",
    "train_data = create_cbow_data(words_idx, window_size)\n",
    "print(f\"Total training pairs: {len(train_data):,}\")\n",
    "print(f\"Sample pair: context={[idx_to_word[idx] for idx in train_data[0][0]]}, target={idx_to_word[train_data[0][1]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fd994",
   "metadata": {},
   "source": [
    "### Define CBOW Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "525f1475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with 71,290 vocabulary size and 100 embedding dimensions\n",
      "Total parameters: 14,329,290\n"
     ]
    }
   ],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \"\"\"Continuous Bag of Words model for Word2Vec.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        # Embedding layer: maps word indices to dense vectors\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Linear layer: maps from embedding space to vocabulary\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, context):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            context: Tensor of shape (batch_size, context_size) containing context word indices\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        # Average the context word embeddings\n",
    "        # context shape: (batch_size, context_size)\n",
    "        embeds = self.embeddings(context)  # (batch_size, context_size, embedding_dim)\n",
    "        embeds = embeds.mean(dim=1)  # (batch_size, embedding_dim) - average pooling\n",
    "        # Project to vocabulary size\n",
    "        out = self.linear(embeds)  # (batch_size, vocab_size)\n",
    "        return out\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_dim = 100\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "print(f\"Model created with {vocab_size:,} vocabulary size and {embedding_dim} embedding dimensions\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aafe2e",
   "metadata": {},
   "source": [
    "### Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f905800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete!\n",
      "Epochs: 1\n",
      "Batch size: 128\n",
      "Learning rate: 0.001\n",
      "Total batches per epoch: 130,616\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Create dataset and data loader\n",
    "dataset = CBOWDataset(train_data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Training setup complete!\")\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Total batches per epoch: {len(dataloader):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474bc43f",
   "metadata": {},
   "source": [
    "### Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004851a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/1, Batch 10000/130616, Loss: 6.6782\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (context_batch, target_batch) in enumerate(dataloader):\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(context_batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, target_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 10000 batches\n",
    "        if (batch_idx + 1) % 10000 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed in {elapsed_time:.2f}s, Average Loss: {avg_loss:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1cf35",
   "metadata": {},
   "source": [
    "### Visualize Training Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('CBOW Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e26bb6",
   "metadata": {},
   "source": [
    "### Extract Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract learned word embeddings\n",
    "model.eval()\n",
    "embeddings = model.embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "print(f\"Embedding matrix shape: {embeddings.shape}\")\n",
    "print(f\"Sample embedding for 'the': {embeddings[word_to_idx.get('the', 0)][:10]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4f909",
   "metadata": {},
   "source": [
    "### Visualize Word Embeddings (t-SNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6badd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings using t-SNE\n",
    "# Select a subset of common words for visualization\n",
    "common_words = ['the', 'of', 'and', 'in', 'to', 'a', 'is', 'that', 'for', 'it',\n",
    "                'as', 'was', 'with', 'be', 'on', 'not', 'he', 'i', 'this', 'but',\n",
    "                'from', 'they', 'she', 'or', 'an', 'will', 'my', 'one', 'all', 'would',\n",
    "                'there', 'their', 'what', 'so', 'up', 'out', 'if', 'about', 'who', 'get']\n",
    "\n",
    "# Get embeddings for common words\n",
    "word_list = []\n",
    "embedding_list = []\n",
    "\n",
    "for word in common_words:\n",
    "    if word in word_to_idx:\n",
    "        word_list.append(word)\n",
    "        embedding_list.append(embeddings[word_to_idx[word]])\n",
    "\n",
    "if len(embedding_list) > 0:\n",
    "    embedding_array = np.array(embedding_list)\n",
    "    \n",
    "    # Apply t-SNE for dimensionality reduction to 2D\n",
    "    print(\"Applying t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(word_list)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embedding_array)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
    "    \n",
    "    # Annotate points with words\n",
    "    for i, word in enumerate(word_list):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    fontsize=9, alpha=0.7)\n",
    "    \n",
    "    plt.title('Word Embeddings Visualization (t-SNE)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No common words found in vocabulary for visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a60ce2",
   "metadata": {},
   "source": [
    "### Find Similar Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23618bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word, embeddings, word_to_idx, idx_to_word, top_k=10):\n",
    "    \"\"\"Find most similar words using cosine similarity.\"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"Word '{word}' not in vocabulary.\")\n",
    "        return\n",
    "    \n",
    "    # Get embedding for the query word\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_embedding = embeddings[word_idx]\n",
    "    \n",
    "    # Calculate cosine similarity with all words\n",
    "    # Normalize embeddings\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized_embeddings = embeddings / (norms + 1e-8)\n",
    "    word_embedding_norm = word_embedding / (np.linalg.norm(word_embedding) + 1e-8)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = np.dot(normalized_embeddings, word_embedding_norm)\n",
    "    \n",
    "    # Get top k similar words (excluding the word itself)\n",
    "    top_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "    \n",
    "    print(f\"Words similar to '{word}':\")\n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        print(f\"{i}. {idx_to_word[idx]} (similarity: {similarities[idx]:.4f})\")\n",
    "\n",
    "# Test with some example words\n",
    "test_words = ['king', 'queen', 'man', 'woman', 'good', 'bad']\n",
    "for word in test_words:\n",
    "    if word in word_to_idx:\n",
    "        find_similar_words(word, embeddings, word_to_idx, idx_to_word)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b8842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
